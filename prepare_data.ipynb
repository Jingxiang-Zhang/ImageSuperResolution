{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa0c4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import crop, pad\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf48f90",
   "metadata": {},
   "source": [
    "#### path define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ca4d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"D:\\programming\\dataset\\DIV2K\" # base dir\n",
    "# original high resolution training data folder\n",
    "train_hr = os.path.join(base_path, \"DIV2K_train_HR\") \n",
    "# original low resolution training label folder\n",
    "train_lr = os.path.join(base_path, \"DIV2K_train_LR_bicubic_X2\")\n",
    "# original high resolution test data folder\n",
    "val_hr = os.path.join(base_path, \"DIV2K_valid_HR\")\n",
    "# original low resolution test label folder\n",
    "val_lr = os.path.join(base_path, \"DIV2K_valid_LR_bicubic_X2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a42e67",
   "metadata": {},
   "source": [
    "#### prepare for data set\n",
    "\n",
    "This dataset use to test how the patches size influence the training, Set LR patches size = 16, 32, 64, 128, 256, corresponding HR patches size = 32, 64, 128, 256, 512, use padding with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59001413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Y_channel(hr_path, lr_path, h5_folder, patch_size):\n",
    "    \"\"\"\n",
    "    create train data by the data set folder created by crop_image_LR_to_4_patches\n",
    "    and crop_image_HR_to_4_patches\n",
    "    \"\"\"\n",
    "    if not os.path.exists(h5_folder):\n",
    "        os.makedirs(h5_folder)\n",
    "    length_list = list()\n",
    "    for number in range(6):\n",
    "        image_number_range = range(number * 100 + 1, number * 100 + 101)\n",
    "        h5path = os.path.join(h5_folder, str(number) + \".hdf5\")\n",
    "        length = create_Y_channel_core(hr_path, lr_path, h5path, image_number_range, patch_size)\n",
    "        length_list.append(length)\n",
    "    index_file = os.path.join(h5_folder, \"index.txt\")\n",
    "    with open(index_file, \"w\") as file:\n",
    "        for length in length_list:\n",
    "            file.write(str(length) + \"\\n\")\n",
    "         \n",
    "        \n",
    "def create_Y_channel_core(hr_path, lr_path, h5path, image_number_range, patch_size):\n",
    "    \"\"\"\n",
    "    called by create_train_data_Y_channel, read 100 numbers of LR and HR images,\n",
    "    save to hdf5 file\n",
    "    \"\"\"\n",
    "    hrPatches = list()\n",
    "    lrPatches = list()\n",
    "    with tqdm(total=len(image_number_range)) as t:\n",
    "        for image_number in image_number_range:\n",
    "            hrp = os.path.join(hr_path, \"{:0>4}\".format(image_number) + \".png\")\n",
    "            lrp = os.path.join(lr_path, \"{:0>4}\".format(image_number) + \"x2.png\")\n",
    "            if not os.path.exists(hrp):\n",
    "                t.update(1)\n",
    "                continue\n",
    "            hr = cv2.imread(hrp)\n",
    "            hr = cv2.cvtColor(hr, cv2.COLOR_BGR2YCR_CB) # for hr, change to ycbcr\n",
    "            lr = cv2.imread(lrp)\n",
    "            lr = cv2.cvtColor(lr, cv2.COLOR_BGR2YCR_CB) # for hr, change to ycbcr\n",
    "                \n",
    "            hr = np.array(hr).astype(np.float32)[:, :, 0] / 255  # get Y channel\n",
    "            lr = np.array(lr).astype(np.float32)[:, :, 0] / 255  # get Y channel\n",
    "            \n",
    "            height, width = lr.shape\n",
    "            for i in range(0, height, patch_size):\n",
    "                for j in range(0, width, patch_size):\n",
    "                    l_patch_temp = lr[i:i + patch_size, j:j + patch_size]\n",
    "                    w, h = l_patch_temp.shape\n",
    "                    l_patch = np.zeros((patch_size, patch_size), dtype=np.float32)\n",
    "                    l_patch[:w, :h] = l_patch_temp\n",
    "                    l_patch = np.expand_dims(l_patch, axis=0)\n",
    "                    \n",
    "                    h_patch_temp = hr[i*2:i*2 + patch_size*2, j*2:j*2 + patch_size*2]\n",
    "                    w, h = h_patch_temp.shape\n",
    "                    h_patch = np.zeros((patch_size*2, patch_size*2), dtype=np.float32)\n",
    "                    h_patch[:w, :h] = h_patch_temp\n",
    "                    h_patch = np.expand_dims(h_patch, axis=0)\n",
    "                    \n",
    "                    lrPatches.append(l_patch)\n",
    "                    hrPatches.append(h_patch)\n",
    "            t.update(1)\n",
    "            \n",
    "    length = len(hrPatches)\n",
    "    h5_file = h5py.File(h5path, 'w')\n",
    "    h5_file.create_dataset('lr', data=np.array(lrPatches))\n",
    "    h5_file.create_dataset('hr', data=np.array(hrPatches))\n",
    "    h5_file.close()\n",
    "    return length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb8ceec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# create Y channel size = 16\n",
    "h5_folder = os.path.join(base_path, \"train_Ychannel_size_64_128\") \n",
    "create_Y_channel(train_hr, train_lr, h5_folder, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d7cd0",
   "metadata": {},
   "source": [
    "This dataset use to test how the training channel influence the training. Set R, G, B, and RGB three types of channels. patch size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60459aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_BGR_channel(hr_path, lr_path, h5_folder, patch_size):\n",
    "    \"\"\"\n",
    "    create train data by the data set folder created by crop_image_LR_to_4_patches\n",
    "    and crop_image_HR_to_4_patches\n",
    "    \"\"\"\n",
    "    if not os.path.exists(h5_folder):\n",
    "        os.makedirs(h5_folder)\n",
    "    length_list = list()\n",
    "    for number in range(6):\n",
    "        image_number_range = range(number * 100 + 1, number * 100 + 101)\n",
    "        h5path = os.path.join(h5_folder, str(number) + \".hdf5\")\n",
    "        length = create_BGR_channel_core(hr_path, lr_path, h5path, image_number_range, patch_size)\n",
    "        length_list.append(length)\n",
    "    index_file = os.path.join(h5_folder, \"index.txt\")\n",
    "    with open(index_file, \"w\") as file:\n",
    "        for length in length_list:\n",
    "            file.write(str(length) + \"\\n\")\n",
    "         \n",
    "        \n",
    "def create_BGR_channel_core(hr_path, lr_path, h5path, image_number_range, patch_size):\n",
    "    \"\"\"\n",
    "    called by create_train_data_Y_channel, read 100 numbers of LR and HR images,\n",
    "    save to hdf5 file\n",
    "    \"\"\"\n",
    "    hrPatches = list()\n",
    "    lrPatches = list()\n",
    "    with tqdm(total=len(image_number_range)) as t:\n",
    "        for image_number in image_number_range:\n",
    "            hrp = os.path.join(hr_path, \"{:0>4}\".format(image_number) + \".png\")\n",
    "            lrp = os.path.join(lr_path, \"{:0>4}\".format(image_number) + \"x2.png\")\n",
    "            if not os.path.exists(hrp):\n",
    "                t.update(1)\n",
    "                continue\n",
    "            hr = cv2.imread(hrp)\n",
    "            lr = cv2.imread(lrp)                \n",
    "            hr = np.array(hr).astype(np.float32) / 255  # get BGR channel\n",
    "            lr = np.array(lr).astype(np.float32) / 255  # get BGR channel\n",
    "            height, width, _ = lr.shape\n",
    "            for i in range(0, height, patch_size):\n",
    "                for j in range(0, width, patch_size):\n",
    "                    l_patch_temp = lr[i:i + patch_size, j:j + patch_size]\n",
    "                    w, h, _ = l_patch_temp.shape\n",
    "                    l_patch = np.zeros((patch_size, patch_size, 3), dtype=np.float32)\n",
    "                    l_patch[:w, :h] = l_patch_temp\n",
    "                    l_patch = np.transpose(l_patch, [2,0,1])\n",
    "                    \n",
    "                    \n",
    "                    h_patch_temp = hr[i*2:i*2 + patch_size*2, j*2:j*2 + patch_size*2]\n",
    "                    w, h, _ = h_patch_temp.shape\n",
    "                    h_patch = np.zeros((patch_size*2, patch_size*2, 3), dtype=np.float32)\n",
    "                    h_patch[:w, :h] = h_patch_temp\n",
    "                    h_patch = np.transpose(h_patch, [2,0,1])\n",
    "                    \n",
    "                    lrPatches.append(l_patch)\n",
    "                    hrPatches.append(h_patch)\n",
    "            t.update(1)\n",
    "    length = len(hrPatches)\n",
    "    h5_file = h5py.File(h5path, 'w')\n",
    "    h5_file.create_dataset('lr', data=np.array(lrPatches))\n",
    "    h5_file.create_dataset('hr', data=np.array(hrPatches))\n",
    "    h5_file.close()\n",
    "    return length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11ce5f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  8.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:11<00:00,  9.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.57it/s]\n"
     ]
    }
   ],
   "source": [
    "h5_folder = os.path.join(base_path, \"train_BGR_channel\") \n",
    "create_BGR_channel(train_hr, train_lr, h5_folder, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5ec53",
   "metadata": {},
   "source": [
    "This dataset use to test how the scale influence the pre-upsampling performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "19c06748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Y_channel_bicubic(hr_path, h5_folder, patch_size, scale=2):\n",
    "    \"\"\"\n",
    "    create train data by the data set folder created by crop_image_LR_to_4_patches\n",
    "    and crop_image_HR_to_4_patches\n",
    "    \"\"\"\n",
    "    if not os.path.exists(h5_folder):\n",
    "        os.makedirs(h5_folder)\n",
    "    length_list = list()\n",
    "    for number in range(6):\n",
    "        image_number_range = range(number * 100 + 1, number * 100 + 101)\n",
    "        h5path = os.path.join(h5_folder, str(number) + \".hdf5\")\n",
    "        length = create_Y_channel_bicubic_core(hr_path, h5path, image_number_range, patch_size, scale)\n",
    "        length_list.append(length)\n",
    "    index_file = os.path.join(h5_folder, \"index.txt\")\n",
    "    with open(index_file, \"w\") as file:\n",
    "        for length in length_list:\n",
    "            file.write(str(length) + \"\\n\")\n",
    "         \n",
    "        \n",
    "def create_Y_channel_bicubic_core(hr_path, h5path, image_number_range, patch_size, scale):\n",
    "    \"\"\"\n",
    "    called by create_train_data_Y_channel, read 100 numbers of LR and HR images,\n",
    "    save to hdf5 file\n",
    "    \"\"\"\n",
    "    hrPatches = list()\n",
    "    lrPatches = list()\n",
    "    with tqdm(total=len(image_number_range)) as t:\n",
    "        for image_number in image_number_range:\n",
    "            hrp = os.path.join(hr_path, \"{:0>4}\".format(image_number) + \".png\")\n",
    "            if not os.path.exists(hrp):\n",
    "                t.update(1)\n",
    "                continue\n",
    "            \n",
    "            hr = cv2.imread(hrp)\n",
    "            height, width, _ = hr.shape\n",
    "            lr_height, lr_width = height//scale, width//scale\n",
    "            lr = cv2.resize(hr, (lr_width, lr_height), interpolation=cv2.INTER_CUBIC)  # interpolation\n",
    "            lr = cv2.resize(lr, (width, height), interpolation=cv2.INTER_CUBIC)  # interpolation\n",
    "            \n",
    "            hr = cv2.cvtColor(hr, cv2.COLOR_BGR2YCR_CB) # for hr, change to ycbcr\n",
    "            lr = cv2.cvtColor(lr, cv2.COLOR_BGR2YCR_CB) # for lr, change to ycbcr\n",
    "            hr = np.array(hr[:,:,0]).astype(np.float32) / 255  # get Y channel\n",
    "            lr = np.array(lr[:,:,0]).astype(np.float32) / 255  # get Y channel\n",
    "            \n",
    "        \n",
    "            for i in range(0, height, patch_size):\n",
    "                for j in range(0, width, patch_size):\n",
    "                    l_patch_temp = lr[i:i + patch_size, j:j + patch_size]\n",
    "                    w, h = l_patch_temp.shape\n",
    "                    l_patch = np.zeros((patch_size, patch_size), dtype=np.float32)\n",
    "                    l_patch[:w, :h] = l_patch_temp\n",
    "                    l_patch = np.expand_dims(l_patch, [0])\n",
    "                    \n",
    "                    h_patch_temp = hr[i:i + patch_size, j:j + patch_size]\n",
    "                    w, h = h_patch_temp.shape\n",
    "                    h_patch = np.zeros((patch_size, patch_size), dtype=np.float32)\n",
    "                    h_patch[:w, :h] = h_patch_temp\n",
    "                    h_patch = np.expand_dims(h_patch, [0])\n",
    "                    \n",
    "                    lrPatches.append(l_patch)\n",
    "                    hrPatches.append(h_patch)\n",
    "            t.update(1)\n",
    "    length = len(hrPatches)\n",
    "    h5_file = h5py.File(h5path, 'w')\n",
    "    h5_file.create_dataset('lr', data=np.array(lrPatches))\n",
    "    h5_file.create_dataset('hr', data=np.array(hrPatches))\n",
    "    h5_file.close()\n",
    "    return length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ff21bb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 11.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 11.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.65it/s]\n"
     ]
    }
   ],
   "source": [
    "h5_folder = os.path.join(base_path, \"train_scale3_channel\") \n",
    "create_Y_channel_bicubic(train_hr, h5_folder, 128, scale=3)\n",
    "h5_folder = os.path.join(base_path, \"train_scale4_channel\") \n",
    "create_Y_channel_bicubic(train_hr, h5_folder, 128, scale=4)\n",
    "h5_folder = os.path.join(base_path, \"train_scale5_channel\") \n",
    "create_Y_channel_bicubic(train_hr, h5_folder, 128, scale=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564811b9",
   "metadata": {},
   "source": [
    "#### history code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fb347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous code, no need anymore\n",
    "\n",
    "def create_train_data(hr_path, lr_path, h5path, pSize=33, pStride=33, padding=6):\n",
    "    h5_file = h5py.File(h5path, 'w')\n",
    "    hrPatches = list()\n",
    "    lrPatches = list()\n",
    "    for i in range(301, 551):\n",
    "        hrp = os.path.join(hr_path, \"{:0>4}\".format(i) + \".png\")\n",
    "        lrp = os.path.join(lr_path, \"{:0>4}\".format(i) + \"x2.png\")\n",
    "        hr = cv2.imread(hrp)\n",
    "        hr = cv2.cvtColor(hr, cv2.COLOR_BGR2YCR_CB) # for hr, change to ycbcr\n",
    "        \n",
    "        lr = cv2.imread(lrp)\n",
    "        lr = cv2.cvtColor(lr, cv2.COLOR_BGR2RGB)\n",
    "        lr = cv2.resize(lr, (hr.shape[1], hr.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "        lr = cv2.cvtColor(lr, cv2.COLOR_RGB2YCR_CB)\n",
    "        \n",
    "        hr = np.array(hr).astype(np.float32)[:,:,0] / 255 # get Y channel\n",
    "        lr = np.array(lr).astype(np.float32)[:,:,0] / 255 # get Y channel\n",
    "        height, width = hr.shape\n",
    "        for i in range(0, height - pSize + 1, pStride):\n",
    "            for j in range(0, width - pSize + 1, pStride):\n",
    "                l = lr[i:i + pSize, j:j + pSize]\n",
    "                l = np.reshape(l,newshape=(1, pSize, pSize))\n",
    "                lrPatches.append(l)\n",
    "                h = hr[i + padding:i - padding + pSize, j + padding:j + pSize - padding]\n",
    "                h = np.reshape(h,newshape=(1, pSize-padding*2, pSize-padding*2))\n",
    "                hrPatches.append(h)\n",
    "    h5_file.create_dataset('lr', data=np.array(lrPatches))\n",
    "    h5_file.create_dataset('hr', data=np.array(hrPatches))\n",
    "    h5_file.close()\n",
    "    \n",
    "\n",
    "# create validation set with RGB\n",
    "\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, padding):\n",
    "        super(ValDataset, self).__init__()\n",
    "        self.padding = padding\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx + 551\n",
    "        hrp = os.path.join(train_hr, \"{:0>4}\".format(idx) + \".png\") # open the image of \n",
    "        lrp = os.path.join(train_lr, \"{:0>4}\".format(idx) + \"x2.png\") # the val set\n",
    "        hr = cv2.imread(hrp)\n",
    "        hr = cv2.cvtColor(hr, cv2.COLOR_BGR2YCR_CB) # for hr, change to ycbcr\n",
    "        \n",
    "        lr = cv2.imread(lrp)\n",
    "        lr = cv2.cvtColor(lr, cv2.COLOR_BGR2RGB)\n",
    "        lr = cv2.resize(lr, (hr.shape[1], hr.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "        lr = cv2.cvtColor(lr, cv2.COLOR_RGB2YCR_CB)\n",
    "        \n",
    "        hr = hr[self.padding:-self.padding,self.padding:-self.padding]\n",
    "        hr = np.array(hr).astype(np.float32) / 255\n",
    "        lr = np.array(lr).astype(np.float32) / 255\n",
    "        return hr, lr\n",
    "    \n",
    "\n",
    "def test_psnr_interpolation():\n",
    "    padding = 6\n",
    "    psnr_list = list()\n",
    "    ssim_list = list()\n",
    "    \n",
    "    for hr, lr in val_dataloader:\n",
    "        hr = hr.numpy() * 255\n",
    "        hr = np.reshape(hr, (hr.shape[1], hr.shape[2], hr.shape[3]))\n",
    "        hr = np.array(hr).astype(np.uint8)\n",
    "        hr = cv2.cvtColor(hr, cv2.COLOR_YCR_CB2RGB)\n",
    "\n",
    "        lr = lr.numpy() * 255\n",
    "        lr = np.reshape(lr, (lr.shape[1], lr.shape[2], lr.shape[3]))\n",
    "        lr = lr[padding:-padding,padding:-padding]\n",
    "        lr = np.array(lr).astype(np.uint8)\n",
    "        lr = cv2.cvtColor(lr, cv2.COLOR_YCR_CB2RGB)\n",
    "        psnr = PSNR(hr, lr, 255)\n",
    "        psnr_list.append(psnr)\n",
    "        ssim = SSIM(hr, lr)\n",
    "        ssim_list.append(ssim)\n",
    "    return np.average(psnr_list), np.average(ssim_list)\n",
    "\n",
    "# psnr, ssim = test_psnr_interpolation()\n",
    "# print(\"psnr for interpolation is: {:.3f}\".format(psnr))\n",
    "# print(\"ssim for interpolation is: {:.3f}\".format(ssim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
